{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Llama 3.2 3B using unsloth and function calling\n",
    "\n",
    "Llama 3.2 prompt format\n",
    "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penhfel/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelipe-fonseca\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/penhfel/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "import wandb\n",
    "with open(folder_path+'../tokens.json', 'r') as file_handler:\n",
    "    tokens = json.load(file_handler)\n",
    "\n",
    "# Defined in the secrets tab in Google Colab\n",
    "hf_token = tokens['hf_token']\n",
    "wb_token = tokens['wandb_token']\n",
    "wandb.login(key=wb_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4080. Max memory: 15.992 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.24. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token, # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_with_tools = \"\"\"You are an expert in composing functions. You are given a question and a set of possible functions. \n",
    "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
    "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
    "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
    "You SHOULD NOT include any other text in the response.\n",
    "Here is a list of functions in JSON format that you can invoke.[\n",
    "    {\n",
    "        \"name\": \"get_user_info\",\n",
    "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"dict\",\n",
    "            \"required\": [\n",
    "                \"user_id\"\n",
    "            ],\n",
    "            \"properties\": {\n",
    "                \"user_id\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
    "            },\n",
    "            \"special\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
    "                \"default\": \"none\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\"\"\"\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_with_tools},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Can you retrieve the details for the user with the ID 7890, who has black as their special request?\",\n",
    "            }\n",
    "        ]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are an expert in composing functions. You are given a question and a set of possible functions. \n",
      "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
      "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
      "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
      "You SHOULD NOT include any other text in the response.\n",
      "Here is a list of functions in JSON format that you can invoke.[\n",
      "    {\n",
      "        \"name\": \"get_user_info\",\n",
      "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"dict\",\n",
      "            \"required\": [\n",
      "                \"user_id\"\n",
      "            ],\n",
      "            \"properties\": {\n",
      "                \"user_id\": {\n",
      "                \"type\": \"integer\",\n",
      "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
      "            },\n",
      "            \"special\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
      "                \"default\": \"none\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "]user\n",
      "\n",
      "Can you retrieve the details for the user with the ID 7890, who has black as their special request?assistant\n",
      "\n",
      "[get_user_info(user_id=7890, special='black')]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(**input_ids, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chat_template = \"\"\"{{- bos_token }}\n",
    "{%- if custom_tools is defined %}\n",
    "    {%- set tools = custom_tools %}\n",
    "{%- endif %}\n",
    "{%- if not date_string is defined %}\n",
    "    {%- if strftime_now is defined %}\n",
    "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
    "    {%- else %}\n",
    "        {%- set date_string = \"26 Jul 2024\" %}\n",
    "    {%- endif %}\n",
    "{%- endif %}\n",
    "{%- if not tools is defined %}\n",
    "    {%- set tools = none %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
    "{%- if messages[0]['role'] == 'system' %}\n",
    "    {%- set system_message = messages[0]['content']|trim %}\n",
    "    {%- set messages = messages[1:] %}\n",
    "{%- else %}\n",
    "    {%- set system_message = \"\" %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- System message #}\n",
    "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
    "{%- if tools is not none  %}\n",
    "    {{- \"You are an expert in composing functions. You are given a question and a set of possible functions.\\n\" }}\n",
    "    {{- \"Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\\n\" }}\n",
    "    {{- \"If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\\n\" }}\n",
    "    {{- \"If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)].\\n\" }}\n",
    "    {{- 'You SHOULD NOT include any other text in the response.\\n' }}\n",
    "    {{- 'You SHOULD ONLY choose one of the given functions.\\n' }}\n",
    "    {{- \"Here is a list of functions in JSON format that you can invoke.[\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \",\\n\" }}\n",
    "    {%- endfor %}\n",
    "    {{- \"]\\n\" }}\n",
    "{%- endif %}\n",
    "{{- system_message }}\n",
    "{{- \"<|eot_id|>\" }}\n",
    "\n",
    "{%- for message in messages %}\n",
    "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
    "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
    "    {%- elif 'tool_calls' in message %}\n",
    "        {%- if not message.tool_calls|length == 1 %}\n",
    "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
    "        {%- endif %}\n",
    "        {%- set tool_call = message.tool_calls[0].function %}\n",
    "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
    "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
    "        {{- '\"parameters\": ' }}\n",
    "        {{- tool_call.arguments | tojson }}\n",
    "        {{- \"}\" }}\n",
    "        {{- \"<|eot_id|>\" }}\n",
    "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
    "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
    "        {%- if message.content is mapping or message.content is iterable %}\n",
    "            {{- message.content | tojson }}\n",
    "        {%- else %}\n",
    "            {{- message.content }}\n",
    "        {%- endif %}\n",
    "        {{- \"<|eot_id|>\" }}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "{%- endif %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template =  custom_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_cartao_de_credito(pergunta_sobre_cartao: str):\n",
    "    \"\"\"\n",
    "    Fa√ßa perguntas para um chatbot especializado no assunto de cart√£o de cr√©dito\n",
    "\n",
    "    Args:\n",
    "        pergunta_sobre_cartao: Pergunta do usu√°rio que leva em considera√ß√£o o hist√≥rico da conversa\n",
    "    \"\"\"\n",
    "    return 'I know about credit cards'\n",
    "\n",
    "def chatbot_sobre_transferencia(pergunta_sobre_transf: str):\n",
    "    \"\"\"\n",
    "    Fa√ßa perguntas para um chatbot especializado no assunto de transferencias banc√°rias\n",
    "\n",
    "    Args:\n",
    "        pergunta_sobre_transf: Pergunta do usu√°rio que leva em considera√ß√£o o hist√≥rico da conversa\n",
    "    \"\"\"\n",
    "    return '10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Como eu posso pedir um cart√£o de cr√©dito?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Eu tenho uma lista de cart√µes para te oferecer. Qual desses cart√µes voc√™ quer solicitar? Black, Visa ou Infinite?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"o visa\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, tools=[chatbot_cartao_de_credito,chatbot_sobre_transferencia])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\n",
      "[chatbot_cartao_de_credito(pergunta_sobre_cartao='Como eu posso pedir um cart√£o de cr√©dito?')]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(**input_ids, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0][len(input_ids.input_ids[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
